{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading libraries\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "batch_size = 32\n",
    "word_embedding_size = 80\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JtMGasiEKttA"
   },
   "outputs": [],
   "source": [
    "#Basic preprocessing on the text\n",
    "f = open('war_peace.txt', 'r',encoding='utf8')\n",
    "x = f.read()\n",
    "f.close()\n",
    "x=x.split(\"\\n\\n\")\n",
    "for i in range(0,len(x)):\n",
    "    x[i]=x[i].replace(\"\\n\",\" \")\n",
    "new_text=[]\n",
    "for i in range(0,len(x)):\n",
    "    text=x[i].split(\". \")\n",
    "    for j in range(0,len(text)):\n",
    "        new_text.append(text[j])\n",
    "for i in range(0,len(new_text)):\n",
    "    new_text[i]=new_text[i].lower()\n",
    "new_txt = []\n",
    "for i in range(0,len(new_text)):\n",
    "    if(78>len(new_text[i].split(\" \"))):\n",
    "        new_txt.append(new_text[i])  \n",
    "train_data = []\n",
    "for i in range(0,len(new_txt)):\n",
    "    if len(new_text[i].split())>1:\n",
    "        train_data.append('<start> '+ new_txt[i] + \" <end>\")\n",
    "for i in range(0,len(train_data)):\n",
    "    train_data[i]=train_data[i].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing a train and test split\n",
    "test_data = train_data[26773:]\n",
    "train_data = train_data[:26773]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P6Ln5_q5KttN"
   },
   "outputs": [],
   "source": [
    "##########loading the word2vec models from memory #################\n",
    "#model_w2v = Word2Vec(train_data,size = word_embedding_size,window = 5,min_count = 1,iter = 50)\n",
    "#model_tes = Word2Vec(train_data,size = word_embedding_size,window=5,min_count=2,iter = 10)\n",
    "#model_w2v.save('word2vec.mode')\n",
    "#model_tes.save('word2vec_tes.model')\n",
    "model_w2v = Word2Vec.load(\"word2vec.model\")\n",
    "model_w2v_tes = Word2Vec.load(\"word2vec_tes.model\")\n",
    "vocab = list(model_w2v.wv.vocab)\n",
    "a = list(range(1,len(vocab)+1))\n",
    "vocab_test = list(model_w2v_tes.wv.vocab)\n",
    "vocab_dict = dict(zip(a,vocab))\n",
    "vocab_dict_inv = dict(zip(vocab,a))\n",
    "dif = list(set(vocab_test) - set(list(model_w2v.wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to convert the data into fixed length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(data):\n",
    "    train = np.zeros([len(data),80,1],dtype=np.int64)\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            try:\n",
    "                train[i][j][0] = vocab_dict_inv[data[i][j]]\n",
    "            except KeyError:\n",
    "                train[i][j][0] = vocab_dict_inv[random.choice(dif)]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## invoking the conv function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = conv(train_data)\n",
    "final_test = conv(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## instead of padding all the words to max length in the dataset just padding the sentenses to max length in batch\n",
    "## this function also converts words to word2vec embeddings\n",
    "def proces_on_batch(data):\n",
    "    data_update = []\n",
    "    data = np.array(data)\n",
    "    for i in range(len(data)):\n",
    "        data_update.append(list(np.array(data[i]).reshape([80]))[:list(np.array(data[i]).reshape([80])).index(4) + 1])\n",
    "    \n",
    "    max_len_in_batch = len(max(data_update, key=len))\n",
    "    \n",
    "    train = np.zeros([batch_size,max_len_in_batch,word_embedding_size])\n",
    "    target = np.zeros([batch_size,max_len_in_batch,len(vocab)+1])\n",
    "    for k in range(0,batch_size):\n",
    "        for m in range(max_len_in_batch):\n",
    "            target[k][m][0] = 1\n",
    "    zeros = np.zeros([word_embedding_size])\n",
    "    \n",
    "    for i in range(len(data_update)):\n",
    "        for j in range(len(data_update[i])-1):\n",
    "            target[i][j][0] = 0\n",
    "            target[i][j][data_update[i][j+1]] = 1\n",
    "            train[i][j] = model_w2v.wv[vocab_dict[data_update[i][j]]]\n",
    "            \n",
    "    return train,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating the data input pipe line and creating the iterators\n",
    "dataset = tf.data.Dataset.from_tensor_slices(final_train)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(final_test)\n",
    "dataset = dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "test_iterator = test_dataset.make_one_shot_iterator()\n",
    "iterator = dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initializing the optimizer\n",
    "optimzer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM states for use during inference\n",
    "lstm_1_ht = tf.contrib.eager.Variable(np.zeros([1,128]),dtype=tf.float32)\n",
    "lstm_1_ct = tf.contrib.eager.Variable(np.zeros([1,128]),dtype=tf.float32)\n",
    "lstm_2_ht = tf.contrib.eager.Variable(np.zeros([1,128]),dtype=tf.float32)\n",
    "lstm_2_ct = tf.contrib.eager.Variable(np.zeros([1,128]),dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## keras.Model class definition and all the variables in this class will be trained by the optimizer\n",
    "class language_model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(tf.keras.Model,self).__init__()\n",
    "        self.LSTM_1 = tf.keras.layers.LSTM(128,return_sequences = True,\n",
    "                                        recurrent_initializer= tf.keras.initializers.truncated_normal(stddev=0.1),\n",
    "                                           recurrent_regularizer = tf.keras.regularizers.l2(0.01),\n",
    "                                           kernel_initializer=tf.keras.initializers.truncated_normal(stddev=0.1),\n",
    "                                          bias_initializer='zeros',kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                                           bias_regularizer = tf.keras.regularizers.l2(0.01),return_state = True)\n",
    "        \n",
    "        self.LSTM_2 = tf.keras.layers.LSTM(128,return_sequences = True,\n",
    "                                           recurrent_initializer = tf.keras.initializers.truncated_normal(stddev=0.1),\n",
    "                                           recurrent_regularizer = tf.keras.regularizers.l2(0.01),\n",
    "                                           kernel_initializer=tf.keras.initializers.truncated_normal(stddev=0.1),\n",
    "                                          bias_initializer='zeros',kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                                           bias_regularizer = tf.keras.regularizers.l2(0.01),return_state = True)\n",
    "        \n",
    "        \n",
    "        self.out = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(len(vocab)+1,\n",
    "                                                kernel_initializer=tf.keras.initializers.truncated_normal(stddev=0.1),\n",
    "                                                bias_initializer='zeros',\n",
    "                                                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                                               bias_regularizer = tf.keras.regularizers.l2(0.01)))\n",
    "        \n",
    "        self.lstm1_ht = tf.contrib.eager.Variable(np.zeros([batch_size,128]),dtype=tf.float32,name='LSTM_1_ht')\n",
    "        self.lstm1_ct = tf.contrib.eager.Variable(np.zeros([batch_size,128]),dtype=tf.float32,name='LSTM_1_ct')\n",
    "        self.lstm2_ht = tf.contrib.eager.Variable(np.zeros([batch_size,128]),dtype=tf.float32,name='LSTM_2_ht')\n",
    "        self.lstm2_ct = tf.contrib.eager.Variable(np.zeros([batch_size,128]),dtype=tf.float32,name='LSTM_2_ct')\n",
    "        \n",
    "    def main_model(self,train_values,state):\n",
    "        \n",
    "        global lstm_1_ht\n",
    "        global lstm_1_ct\n",
    "        global lstm_2_ht\n",
    "        global lstm_2_ct\n",
    "        \n",
    "        if state == \"train\":\n",
    "            x,_,_ = self.LSTM_1(train_values,initial_state = [self.lstm1_ht,self.lstm1_ct] )\n",
    "            x,_,_ = self.LSTM_2(x,initial_state = [self.lstm2_ht,self.lstm2_ct] )\n",
    "            x = self.out(x)\n",
    "            \n",
    "            return x\n",
    "        \n",
    "        else:\n",
    "            x,lstm_1_ht,lstm_1_ct = self.LSTM_1(train_values,initial_state = [lstm_1_ht,lstm_1_ct])\n",
    "            x,lstm_2_ht,lstm_2_ct = self.LSTM_2(x,initial_state = [lstm_2_ht,lstm_2_ct] )\n",
    "            x = self.out(x)\n",
    "            \n",
    "            return x\n",
    "        \n",
    "model = language_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loss function definition need to use gradient tape which calculating the gradients and then passing the gradients to optimizer\n",
    "def loss_fun(train_batch,target):\n",
    "    with tf.GradientTape() as t:\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=target,logits=model.main_model(train_batch,\"train\")))\n",
    "    grads = t.gradient(loss,model.variables)\n",
    "    optimzer.apply_gradients(zip(grads,model.variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from jai_model_v2/weights41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=41>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restoring the weights if required\n",
    "model.main_model(tf.zeros([batch_size,60,word_embedding_size]),state=\"train\") ## just for the LSTM weights to be initialized so that values can be restored\n",
    "tf.contrib.eager.Saver.restore(file_prefix='jai_model_v2/weights49',self=tf.contrib.eager.Saver(var_list=list(model.variables)))\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "global_step.assign(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a summary writer for writing files for tensorboard\n",
    "writer = tf.contrib.summary.create_file_writer('loss_lm')\n",
    "writer.set_as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to capture loss (scalar summary) for tensorboard visualization\n",
    "def loss_viz(epoch_training_loss):\n",
    "    with tf.contrib.summary.always_record_summaries():\n",
    "        tf.contrib.summary.scalar(\"per_epoch_training_loss\",epoch_training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for calculating perplexity\n",
    "def perplexity(test_temp=1,total=0,word_count = 0):\n",
    "    test_iterator = test_dataset.make_one_shot_iterator()\n",
    "    while test_temp == 1:\n",
    "        try:\n",
    "            test,test_target = proces_on_batch(test_iterator.get_next())\n",
    "            test = tf.cast(test,dtype=tf.float32)\n",
    "            test_target = tf.cast(test_target,dtype=tf.float32)\n",
    "            temp = model.main_model(test,state=\"train\")\n",
    "            temp = tf.nn.softmax(temp)\n",
    "            for i in range(len(np.array(temp))):\n",
    "                for j in range(len(np.array(temp[i]))):\n",
    "                    if np.argmax(test_target[i][j]) == 0:\n",
    "                        break\n",
    "                    else:\n",
    "                        total += np.log2(np.array(temp[i][j][np.argmax(test_target[i][j])]))\n",
    "                        word_count += 1\n",
    "        \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            total = -(total/word_count)\n",
    "            perplexity = pow(2,total)\n",
    "            print(\"perplexity\" , perplexity)\n",
    "            prep_list.append(total)\n",
    "            test_temp = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training loop\n",
    "## once the iterator is done with one batch it will throw a OutOfRange exception will catch the exception and restart iterator\n",
    "## initializable iterator is not supported in tensorflow eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for epoch  43  is:  1002.3931287527084\n",
      "perplexity 2311.1707645529314\n",
      "loss for epoch  44  is:  995.1360294222832\n",
      "perplexity 2401.9244490572414\n",
      "loss for epoch  45  is:  989.8590504527092\n",
      "perplexity 2491.9783127131504\n",
      "loss for epoch  46  is:  985.9516336321831\n",
      "perplexity 2573.377593805375\n",
      "loss for epoch  47  is:  981.8186818361282\n",
      "perplexity 2666.511452777533\n",
      "loss for epoch  48  is:  978.0976364016533\n",
      "perplexity 2732.158531044606\n",
      "loss for epoch  49  is:  975.3105364441872\n",
      "perplexity 2839.1227939020164\n"
     ]
    }
   ],
   "source": [
    "iterator = dataset.make_one_shot_iterator()\n",
    "total_loss = 0\n",
    "i = 50\n",
    "while i < 62:\n",
    "    try:\n",
    "        train_batch = iterator.get_next()\n",
    "        train_batch,target = proces_on_batch(train_batch)\n",
    "\n",
    "        train_batch = tf.cast(train_batch,dtype=tf.float32)\n",
    "        target = tf.cast(target,dtype=tf.float32)\n",
    "\n",
    "        loss = loss_fun(train_batch,target)\n",
    "        total_loss += np.array(loss)\n",
    "        \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"loss for epoch \",i,\" is: \",total_loss)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        global_step.assign_add(1)\n",
    "        loss_viz(total_loss)\n",
    "        tf.contrib.eager.Saver.save(file_prefix='jai_model_v2/weights' + str(i),self=tf.contrib.eager.Saver(var_list=list(model.variables)))\n",
    "        i = i + 1\n",
    "        total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inference loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_w2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-65bb08fe0a28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#### initializing with <start> token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mcurrent_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel_w2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<start>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword_embedding_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#### inference function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_w2v' is not defined"
     ]
    }
   ],
   "source": [
    "#### initializing the cell state and hidden state ####\n",
    "lstm_1_ht = tf.reshape(model.lstm1_ht[0],shape=[1,128])\n",
    "lstm_1_ct = tf.reshape(model.lstm1_ct[0],shape=[1,128])\n",
    "lstm_2_ht = tf.reshape(model.lstm2_ht[0],shape=[1,128])\n",
    "lstm_2_ct = tf.reshape(model.lstm2_ct[0],shape=[1,128])\n",
    "h = 1\n",
    "\n",
    "#### initializing with <start> token\n",
    "current_word = (model_w2v.wv['<start>']).reshape([1,1,word_embedding_size])\n",
    "\n",
    "#### inference function\n",
    "def inference(current_word,search):\n",
    "    global h\n",
    "    current_word = model.main_model(current_word,\"inference\")\n",
    "    #### condition for greedy or random search\n",
    "    if search == 'greedy':\n",
    "        current_word = np.random.choice(np.argsort(np.array(((tf.nn.softmax(current_word[0][0])))))[-1:])\n",
    "    else:\n",
    "        current_word = np.random.choice(np.argsort(np.array(((tf.nn.softmax(current_word[0][0])))))[-5:])\n",
    "    if current_word == 0:\n",
    "        current_word = np.zeros([1,1,word_embedding_size])\n",
    "        print(\"<pad>\",end=\" \")\n",
    "    else:\n",
    "        current_word = vocab_dict[current_word]\n",
    "        if current_word == \"<end>\":\n",
    "            print(\"\\n\")\n",
    "            lstm_1_ht = tf.reshape(model.lstm1_ht[h],shape=[1,128])\n",
    "            lstm_1_ct = tf.reshape(model.lstm1_ct[h],shape=[1,128])\n",
    "            lstm_2_ht = tf.reshape(model.lstm2_ht[h],shape=[1,128])\n",
    "            lstm_2_ct = tf.reshape(model.lstm2_ct[h],shape=[1,128])\n",
    "            h += 1\n",
    "\n",
    "        else:\n",
    "            print(current_word,end=\" \")\n",
    "        current_word = (model_w2v.wv[current_word]).reshape([1,1,word_embedding_size])\n",
    "    \n",
    "    return current_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random search inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“i am not going anywhere,” said he, as if afraid about his presence that she could hardly conceal. \n",
      "\n",
      "her brother would never agree for him \n",
      "\n",
      "he would have saved from her journey to her \n",
      "\n",
      "natásha had now changed with his parents, on their own days, but her own being, and her greatness which makes his life \n",
      "\n",
      "pierre had greatly made her best for her and sickness her grief for him to do \n",
      "\n",
      "she was so fond of, she felt himself powerless, or self-reproach with the pleasure and delight and annoyance for his own \n",
      "\n",
      "he "
     ]
    }
   ],
   "source": [
    "for i in range(0,100):\n",
    "    current_word = inference(tf.convert_to_tensor(current_word,dtype=tf.float32),search = 'random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## greedy search inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,100):\n",
    "    current_word = inference(tf.convert_to_tensor(current_word,dtype=tf.float32),search = \"greedy\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
